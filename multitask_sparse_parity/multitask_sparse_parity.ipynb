{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "RLCT Estimation of Multitask Sparse Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install devinterp seaborn torchvision pickleshare wandb plotly einops scikit-learn\n",
    "!git clone https://github.com/ucla-vision/entropy-sgd.git\n",
    "%cd entropy-sgd\n",
    "from python.optim import EntropySGD\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import copy\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from python.optim import EntropySGD\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from itertools import islice, product\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.optim.sgnht import SGNHT\n",
    "\n",
    "PRIMARY, SECONDARY, TERTIARY, QUATERNARY, QUINARY, SENARY = sns.color_palette(\"muted\")[:6]\n",
    "PRIMARY_LIGHT, SECONDARY_LIGHT, TERTIARY_LIGHT, QUATERNARY_LIGHT, QUINARY_LIGHT, SENARY_LIGHT = sns.color_palette(\n",
    "    \"pastel\"\n",
    ")[:6]\n",
    "\n",
    "print(len(sns.color_palette(\"pastel\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTensorDataLoader:\n",
    "    \"\"\"\n",
    "    A DataLoader-like object for a set of tensors that can be much faster than\n",
    "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
    "    the dataset and calls cat (slow).\n",
    "    \"\"\"\n",
    "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
    "        \"\"\"\n",
    "        Initialize a FastTensorDataLoader.\n",
    "\n",
    "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
    "        :param batch_size: batch size to load.\n",
    "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
    "            iterator is created out of this object.\n",
    "\n",
    "        :returns: A FastTensorDataLoader.\n",
    "        \"\"\"\n",
    "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
    "        self.tensors = tensors\n",
    "\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Calculate # batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder > 0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(self.dataset_len, device=self.tensors[0].device)\n",
    "        else:\n",
    "            self.indices = None\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        if self.indices is not None:\n",
    "            indices = self.indices[self.i:self.i+self.batch_size]\n",
    "            batch = tuple(torch.index_select(t, 0, indices) for t in self.tensors)\n",
    "        else:\n",
    "            batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
    "        self.i += self.batch_size\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "\n",
    "def get_batch(n_tasks, n, Ss, codes, sizes, device='cpu', dtype=torch.float32):\n",
    "    \"\"\"Creates batch. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_tasks : int\n",
    "        Number of tasks.\n",
    "    n : int\n",
    "        Bit string length for sparse parity problem.\n",
    "    Ss : list of lists of ints\n",
    "        Subsets of [1, ... n] to compute sparse parities on.\n",
    "    codes : list of int\n",
    "        The subtask indices which the batch will consist of\n",
    "    sizes : list of int\n",
    "        Number of samples for each subtask\n",
    "    device : str\n",
    "        Device to put batch on.\n",
    "    dtype : torch.dtype\n",
    "        Data type to use for input x. Output y is torch.int64.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : torch.Tensor\n",
    "        inputs\n",
    "    y : torch.Tensor\n",
    "        labels\n",
    "    \"\"\"\n",
    "    batch_x = torch.zeros((sum(sizes), n_tasks+n), dtype=dtype, device=device)\n",
    "    batch_y = torch.zeros((sum(sizes),), dtype=torch.int64, device=device)\n",
    "    start_i = 0\n",
    "    for (S, size, code) in zip(Ss, sizes, codes):\n",
    "        if size > 0:\n",
    "            x = torch.randint(low=0, high=2, size=(size, n), dtype=dtype, device=device)\n",
    "            y = torch.sum(x[:, S], dim=1) % 2\n",
    "            x_task_code = torch.zeros((size, n_tasks), dtype=dtype, device=device)\n",
    "            x_task_code[:, code] = 1\n",
    "            x = torch.cat([x_task_code, x], dim=1)\n",
    "            batch_x[start_i:start_i+size, :] = x\n",
    "            batch_y[start_i:start_i+size] = y\n",
    "            start_i += size\n",
    "    return batch_x, batch_y\n",
    "    \n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation, depth, width):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            activation_fn = nn.ReLU\n",
    "        elif activation == 'Tanh':\n",
    "            activation_fn = nn.Tanh\n",
    "        elif activation == 'Sigmoid':\n",
    "            activation_fn = nn.Sigmoid\n",
    "        else:\n",
    "            assert False, f\"Unrecognized activation function identifier: {activation}\"\n",
    "\n",
    "        # create model\n",
    "        layers = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(n_tasks + n, width))\n",
    "                layers.append(activation_fn())\n",
    "            elif i == depth - 1:\n",
    "                layers.append(nn.Linear(width, 2))\n",
    "            else:\n",
    "                layers.append(nn.Linear(width, width))\n",
    "                layers.append(activation_fn())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_loss_zero_one(parameters):\n",
    "    l1_loss = 0\n",
    "    for param in parameters:\n",
    "        l1_loss += torch.min(param.abs(), (param - 1).abs()).sum()\n",
    "    return l1_loss\n",
    "\n",
    "def run(n_tasks,\n",
    "        n,\n",
    "        k,\n",
    "        D,\n",
    "        width,\n",
    "        depth,\n",
    "        activation,\n",
    "        test_points,\n",
    "        test_points_per_task,\n",
    "        steps,\n",
    "        batch_size,\n",
    "        lr,\n",
    "        weight_decay,\n",
    "        device,\n",
    "        dtype,\n",
    "        log_freq,\n",
    "        verbose,\n",
    "        seed):\n",
    "\n",
    "    torch.set_default_dtype(dtype)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    info = {}\n",
    "    models_saved = []\n",
    "\n",
    "    mlp = MLP(activation, depth, width).to(device)\n",
    "    #_log.debug(\"Created model.\")\n",
    "    #_log.debug(f\"Model has {sum(t.numel() for t in mlp.parameters())} parameters\") \n",
    "    info['P'] = sum(t.numel() for t in mlp.parameters())\n",
    "\n",
    "    Ss = []\n",
    "    for _ in range(n_tasks * 10):\n",
    "        S = tuple(sorted(list(random.sample(range(n), k))))\n",
    "        if S not in Ss:\n",
    "            Ss.append(S)\n",
    "        if len(Ss) == n_tasks:\n",
    "            break\n",
    "    assert len(Ss) == n_tasks, \"Couldn't find enough subsets for tasks for the given n, k\"\n",
    "    info['Ss'] = Ss\n",
    "\n",
    "    probs = np.array([1 / n_tasks for _ in range(n_tasks)])\n",
    "    cdf = np.cumsum(probs)\n",
    "\n",
    "    test_batch_sizes = [int(prob * test_points) for prob in probs]\n",
    "    # _log.debug(f\"Total batch size = {sum(batch_sizes)}\")\n",
    "\n",
    "    if D != -1:\n",
    "        samples = np.searchsorted(cdf, np.random.rand(D,))\n",
    "        hist, _ = np.histogram(samples, bins=n_tasks, range=(0, n_tasks-1))\n",
    "        train_x, train_y = get_batch(n_tasks=n_tasks, n=n, Ss=Ss, codes=list(range(n_tasks)), sizes=hist, device='cpu', dtype=dtype)\n",
    "        train_x = train_x.to(device)\n",
    "        train_y = train_y.to(device)\n",
    "        train_loader = FastTensorDataLoader(train_x, train_y, batch_size=min(D, batch_size), shuffle=True)\n",
    "        train_iter = cycle(train_loader)\n",
    "        info['D'] = D\n",
    "    else:\n",
    "        info['D'] = steps * batch_size\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    info['log_steps'] = list()\n",
    "    info['accuracies'] = list()\n",
    "    info['losses'] = list()\n",
    "    info['losses_subtasks'] = dict()\n",
    "    info['accuracies_subtasks'] = dict()\n",
    "    for i in range(n_tasks):\n",
    "        info['losses_subtasks'][str(i)] = list()\n",
    "        info['accuracies_subtasks'][str(i)] = list()\n",
    "    for step in tqdm(range(steps), disable=not verbose):\n",
    "        if step % log_freq == 0:\n",
    "            with torch.no_grad():\n",
    "                x_i, y_i = get_batch(n_tasks=n_tasks, n=n, Ss=Ss, codes=list(range(n_tasks)), sizes=test_batch_sizes, device=device, dtype=dtype)\n",
    "                y_i_pred = mlp(x_i)\n",
    "                labels_i_pred = torch.argmax(y_i_pred, dim=1)\n",
    "                info['accuracies'].append(torch.sum(labels_i_pred == y_i).item() / test_points) \n",
    "                info['losses'].append(loss_fn(y_i_pred, y_i).item())\n",
    "                for i in range(n_tasks):\n",
    "                    x_i, y_i = get_batch(n_tasks=n_tasks, n=n, Ss=[Ss[i]], codes=[i], sizes=[test_points_per_task], device=device, dtype=dtype)\n",
    "                    y_i_pred = mlp(x_i)\n",
    "                    info['losses_subtasks'][str(i)].append(loss_fn(y_i_pred, y_i).item())\n",
    "                    labels_i_pred = torch.argmax(y_i_pred, dim=1)\n",
    "                    info['accuracies_subtasks'][str(i)].append(torch.sum(labels_i_pred == y_i).item() / test_points_per_task)\n",
    "                info['log_steps'].append(step)\n",
    "                models_saved += [copy.deepcopy(mlp)]\n",
    "        optimizer.zero_grad()\n",
    "        if D == -1:\n",
    "            samples = np.searchsorted(cdf, np.random.rand(batch_size,))\n",
    "            hist, _ = np.histogram(samples, bins=n_tasks, range=(0, n_tasks-1))\n",
    "            x, y_target = get_batch(n_tasks=n_tasks, n=n, Ss=Ss, codes=list(range(n_tasks)), sizes=hist, device=device, dtype=dtype)\n",
    "        else:\n",
    "            x, y_target = next(train_iter)\n",
    "        y_pred = mlp(x)\n",
    "        #loss = loss_fn(y_pred, y_target) + l1_loss_zero_one(mlp.parameters())\n",
    "        loss = loss_fn(y_pred, y_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return info, models_saved, loss_fn\n",
    "\n",
    "n_tasks = 10\n",
    "n = 50\n",
    "k = 3\n",
    "\n",
    "D = -1 # -1 for infinite data\n",
    "\n",
    "width = 100\n",
    "depth = 2\n",
    "activation = 'ReLU'\n",
    "    \n",
    "steps = 5000\n",
    "batch_size = 10000\n",
    "lr = 1e-3\n",
    "weight_decay = 0.0\n",
    "test_points = 30000\n",
    "test_points_per_task = 1000\n",
    "    \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float32\n",
    "\n",
    "#log_freq = max(1, steps // 1000)\n",
    "log_freq = 100\n",
    "verbose=True\n",
    "seed = 0\n",
    "runs = 1\n",
    "        \n",
    "info, models_saved, criterion = run(n_tasks,\n",
    "    n, \n",
    "    k, \n",
    "    D, \n",
    "    width, \n",
    "    depth, \n",
    "    activation, \n",
    "    test_points, \n",
    "    test_points_per_task, \n",
    "    steps, \n",
    "    batch_size, \n",
    "    lr, \n",
    "    weight_decay, \n",
    "    device, \n",
    "    dtype, \n",
    "    log_freq, \n",
    "    verbose, \n",
    "    seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devinterp.slt import estimate_learning_coeff_with_summary\n",
    "\n",
    "N_EPOCHS = steps\n",
    "SAVE_EVERY_N_EPOCHS = log_freq\n",
    "\n",
    "Ss = []\n",
    "for _ in range(n_tasks * 10):\n",
    "    S = tuple(sorted(list(random.sample(range(n), k))))\n",
    "    if S not in Ss:\n",
    "        Ss.append(S)\n",
    "    if len(Ss) == n_tasks:\n",
    "        break\n",
    "assert len(Ss) == n_tasks, \"Couldn't find enough subsets for tasks for the given n, k\"\n",
    "\n",
    "probs = np.array([1 / n_tasks for _ in range(n_tasks)])\n",
    "cdf = np.cumsum(probs)\n",
    "test_batch_sizes = [int(prob * test_points) for prob in probs]\n",
    "\n",
    "train_x, train_y = get_batch(n_tasks=n_tasks, n=n, Ss=Ss, codes=list(range(n_tasks)), sizes=test_batch_sizes, device=device, dtype=dtype)\n",
    "train_data = list(zip(train_x, train_y))\n",
    "        \n",
    "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def estimate_rlcts(models, train_loader, criterion, data_length, device, num_draws, num_models):\n",
    "    estimates = {\"sgnht\": [], \"sgld\": []}\n",
    "    for idx, model in enumerate(tqdm(models)):\n",
    "        for method, optimizer_kwargs in [\n",
    "            #(\"sgnht\", {\"lr\": 1e-7, \"diffusion_factor\": 0.01}),\n",
    "            (\"sgld\", {\"lr\": 1e-3, \"localization\": 400.0, \"noise_level\": 1.0}),\n",
    "        ]:\n",
    "            results = estimate_learning_coeff_with_summary(\n",
    "                model,\n",
    "                train_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer_kwargs=optimizer_kwargs,\n",
    "                sampling_method=SGNHT if method == \"sgnht\" else SGLD,\n",
    "                num_chains=2,\n",
    "                num_draws=num_draws,\n",
    "                num_burnin_steps=200,\n",
    "                num_steps_bw_draws=1,\n",
    "                device=device,\n",
    "                seed=0\n",
    "            )\n",
    "            estimate = results[\"llc/mean\"]\n",
    "            estimates[method].append(estimate)\n",
    "    return estimates\n",
    "\n",
    "def obtain_rlct_estimates(train_loader, models_saved, criterion, runs):\n",
    "    num_models = N_EPOCHS // SAVE_EVERY_N_EPOCHS\n",
    "    data_length = len(train_loader)\n",
    "    rlct_estimates = {\"sgnht\": torch.zeros(runs, num_models), \"sgld\": torch.zeros(runs, num_models)}\n",
    "    num_draws = 800\n",
    "\n",
    "    for run in tqdm(range(runs)):\n",
    "        rlct_estimate = estimate_rlcts(\n",
    "            models_saved[num_models * run : num_models * (run + 1)], train_loader, criterion, data_length, device, num_draws, num_models\n",
    "        )\n",
    "        #rlct_estimates[\"sgnht\"][run] = torch.tensor(rlct_estimate[\"sgnht\"])\n",
    "        rlct_estimates[\"sgld\"][run] = torch.tensor(rlct_estimate[\"sgld\"])\n",
    "\n",
    "    rlct_estimates_final = {\"sgnht\": rlct_estimates[\"sgnht\"].mean(dim=0), \"sgld\": rlct_estimates[\"sgld\"].mean(dim=0)}\n",
    "    return rlct_estimates_final\n",
    "\n",
    "rlct_estimates_final = obtain_rlct_estimates(train_loader, models_saved, criterion, runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '0'\n",
    "\n",
    "def plot_losses(train_losses_final, name = ''):\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    x_axis = np.arange(1, N_EPOCHS, SAVE_EVERY_N_EPOCHS)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=PRIMARY)\n",
    "    plt.yscale('log')\n",
    "    ax1.plot(x_axis, train_losses_final, label=\"Train Loss, sgd\", color=PRIMARY)\n",
    "    #ax1.plot(x_axis, test_losses_final, label=\"Test Loss, sgd\", color=PRIMARY_LIGHT)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=PRIMARY)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"losses_\" + name + \"_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "    \n",
    "def plot_subtask_losses(train_losses_subtasks, n_tasks):\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    x_axis = np.arange(1, N_EPOCHS, SAVE_EVERY_N_EPOCHS)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\", color=PRIMARY)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    for task in range(n_tasks):\n",
    "        ax1.plot(x_axis, train_losses_subtasks[str(task)], label=\"Task \" + str(task), color=sns.color_palette(\"muted\")[task])\n",
    "        \n",
    "    ax1.tick_params(axis=\"y\", labelcolor=PRIMARY)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"losses_subtasks_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "    \n",
    "def plot_accuracies(train_accuracies_final, name = ''):\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    x_axis = np.arange(1, N_EPOCHS, SAVE_EVERY_N_EPOCHS)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Accuracy\", color=PRIMARY)\n",
    "    plt.yscale('log')\n",
    "    ax1.plot(x_axis, train_accuracies_final, label=\"Train Accuracy, sgd\", color=PRIMARY)\n",
    "    #ax1.plot(x_axis, test_accuracies_final, label=\"Test Accuracy, sgd\", color=PRIMARY_LIGHT)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=PRIMARY)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"accuracies_\" + name + \"_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "    \n",
    "def plot_subtask_accuracies(train_accuracies_subtasks, n_tasks):\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    x_axis = np.arange(1, N_EPOCHS, SAVE_EVERY_N_EPOCHS)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Accuracy\", color=PRIMARY)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    for task in range(n_tasks):\n",
    "        ax1.plot(x_axis, train_accuracies_subtasks[str(task)], label=\"Task \" + str(task), color=sns.color_palette(\"muted\")[task])\n",
    "        \n",
    "    ax1.tick_params(axis=\"y\", labelcolor=PRIMARY)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"accuracies_subtasks_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "    \n",
    "def plot_rlcts(rlct_estimates_final, dataset, rlct_estimates_final_other = {}):\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    #first_part = np.arange(1, 1001, 10)\n",
    "    \n",
    "    # Create array from 1000 to 50000 with step 100\n",
    "    # Start from 1100 to avoid duplicating 1000\n",
    "    #second_part = np.arange(1001, N_EPOCHS, 100)\n",
    "    \n",
    "    # Combine the two arrays\n",
    "    #x_axis = np.concatenate([first_part, second_part])\n",
    "    x_axis = np.arange(1, N_EPOCHS, SAVE_EVERY_N_EPOCHS)\n",
    "\n",
    "    fig, ax2 = plt.subplots(figsize=(10, 6))\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"Local Learning Coefficient, $\\hat \\lambda$\", color=SECONDARY)\n",
    "    if rlct_estimates_final_other:\n",
    "        ax2.plot(x_axis, rlct_estimates_final_other[\"sgld\"], label=\"summed curve\", color=TERTIARY)\n",
    "    ax2.plot(x_axis, rlct_estimates_final[\"sgld\"], label=\"SGLD, sgd\", color=TERTIARY_LIGHT)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=SECONDARY)\n",
    "    ax2.legend(loc=\"center right\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"rclt_\" + dataset + \"_\" + str(N_EPOCHS) + \"_epochs.png\")\n",
    "\n",
    "train_losses_final = info['losses']\n",
    "train_accuracies_final = info['accuracies']\n",
    "    \n",
    "plot_losses(train_losses_final, dataset)\n",
    "plot_subtask_losses(info['losses_subtasks'], n_tasks)\n",
    "plot_accuracies(train_accuracies_final, dataset)\n",
    "plot_subtask_accuracies(info['accuracies_subtasks'], n_tasks)\n",
    "plot_rlcts(rlct_estimates_final, dataset='full')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_topk_percent_mask(tensor, proportion):\n",
    "    # Step 1: Flatten the tensor\n",
    "    flattened_tensor = tensor.flatten()\n",
    "\n",
    "    # Step 2: Determine K, where K is 20% of the total number of elements\n",
    "    total_elements = flattened_tensor.numel()\n",
    "    K = int(proportion * total_elements)\n",
    "\n",
    "    # Step 3: Find the value of the K-th largest element\n",
    "    topk_values, _ = torch.topk(flattened_tensor, K)\n",
    "    threshold_value = topk_values[-1]\n",
    "\n",
    "    # Step 4: Create a boolean mask of the top K values\n",
    "    return tensor >= threshold_value\n",
    "\n",
    "\n",
    "def unravel_index(index, shape):\n",
    "    out = []\n",
    "    for dim in reversed(shape):\n",
    "        out.append(index % dim)\n",
    "        index = index // dim\n",
    "    return tuple(reversed(out))\n",
    "\n",
    "def ablation_study(model, loss_fn):\n",
    "    \n",
    "    loss_diffs_per_task = []\n",
    "    for index in tqdm(range(n_tasks)):\n",
    "        loss_diffs = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            loss_diffs[name] = torch.zeros(param.shape)\n",
    "            x_i, y_i = get_batch(n_tasks=n_tasks, n=n, Ss=[Ss[index]], codes=[index], sizes=[test_points_per_task], device=device, dtype=dtype)\n",
    "            y_i_pred = model(x_i)\n",
    "            loss_baseline = loss_fn(y_i_pred, y_i).item()\n",
    "            \n",
    "            for idx in range(param.numel()):\n",
    "                with torch.no_grad():\n",
    "                    # Convert flat index i to multi-dimensional index for the original shape\n",
    "                    multi_idx = unravel_index(idx, param.shape)\n",
    "                \n",
    "                    # Save the original weight value\n",
    "                    original_value = param[multi_idx].item()\n",
    "                \n",
    "                    # Set the weight to zero\n",
    "                    param[multi_idx] = 0.0\n",
    "                    \n",
    "                    y_i_ablated = model(x_i)\n",
    "                    loss_ablated = loss_fn(y_i_ablated, y_i).item()\n",
    "                    \n",
    "                    loss_diffs[name][multi_idx] = abs(loss_ablated - loss_baseline)\n",
    "                    \n",
    "                    # Restore the original weight\n",
    "                    param[multi_idx] = original_value\n",
    "                    torch.cuda.empty_cache()\n",
    "        loss_diffs_per_task.append(loss_diffs)\n",
    "                    \n",
    "    return loss_diffs_per_task   \n",
    "\n",
    "# Use the function\n",
    "loss_diffs_per_task = ablation_study(models_saved[-1], criterion)\n",
    "epsilon = .1\n",
    "num_params = sum(p.numel() for p in models_saved[-1].parameters())\n",
    "indices_per_task = []\n",
    "\n",
    "# Analyze results\n",
    "for index in tqdm(range(n_tasks)):\n",
    "    num_weights = 0\n",
    "    print(index)\n",
    "    indices_per_task.append({})\n",
    "    for name in loss_diffs_per_task[index].keys():\n",
    "        print(f\"Layer: {name}\")\n",
    "        indices = loss_diffs_per_task[index][name] > epsilon\n",
    "        indices_per_task[index][name] = indices\n",
    "        print(loss_diffs_per_task[index][name][indices].shape)\n",
    "        num_weights += sum(loss_diffs_per_task[index][name][indices].shape)\n",
    "    print(index, num_weights, num_params)\n",
    "\n",
    "indices_per_task = []\n",
    "# Analyze results\n",
    "for task in tqdm(range(n_tasks)):\n",
    "    \n",
    "    indices_dict = {}\n",
    "    \n",
    "    proportion = 0.3\n",
    "    indices_dict['model.0.weight'] = return_topk_percent_mask(loss_diffs_per_task[task]['model.0.weight'], proportion)\n",
    "    indices_dict['model.0.bias'] = torch.ones(b_0.shape, dtype=torch.bool)\n",
    "    indices_dict['model.2.weight'] = return_topk_percent_mask(loss_diffs_per_task[task]['model.2.weight'], proportion)\n",
    "    indices_dict['model.2.bias'] = torch.ones(b_1.shape, dtype=torch.bool)\n",
    "        \n",
    "    indices_per_task.append(indices_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices_per_task(path_losses_per_task, path_indices_per_task, indices_per_task):\n",
    "    eps = .01\n",
    "    \n",
    "    for task in tqdm(range(n_tasks)):\n",
    "        path_losses = path_losses_per_task[task]\n",
    "        path_indices = path_indices_per_task[task]\n",
    "        \n",
    "        above_eps_indices = path_losses > eps\n",
    "        \n",
    "        for (idx, jdx), (kdx, ldx) in tqdm(path_indices[above_eps_indices]):\n",
    "            indices_per_task[task]['model.0.weight'][idx, jdx] = True \n",
    "            indices_per_task[task]['model.2.weight'][kdx, ldx] = True\n",
    "            \n",
    "    return indices_per_task\n",
    "        \n",
    "\n",
    "def get_path_losses_per_task(loss_diffs_per_task, n_tasks):\n",
    "    path_losses_per_task = []\n",
    "    path_indices_per_task = []\n",
    "    indices_per_task = []\n",
    "\n",
    "    for task in tqdm(range(n_tasks)):\n",
    "        W_0 = loss_diffs_per_task[task]['model.0.weight']\n",
    "        b_0 = loss_diffs_per_task[task]['model.0.bias']\n",
    "        W_1 = loss_diffs_per_task[task]['model.2.weight']\n",
    "        b_1 = loss_diffs_per_task[task]['model.2.bias']\n",
    "        \n",
    "        indices_dict = {}\n",
    "        indices_dict['model.0.weight'] = torch.zeros(W_0.shape, dtype=torch.bool)\n",
    "        indices_dict['model.0.bias'] = torch.ones(b_0.shape, dtype=torch.bool)\n",
    "        indices_dict['model.2.weight'] = torch.zeros(W_1.shape, dtype=torch.bool)\n",
    "        indices_dict['model.2.bias'] = torch.ones(b_1.shape, dtype=torch.bool)\n",
    "        \n",
    "        indices_per_task.append(indices_dict)\n",
    "    \n",
    "        path_losses = []\n",
    "        path_indices = []\n",
    "\n",
    "        for idx in tqdm(range(W_0.shape[0])):\n",
    "            for jdx in range(W_0.shape[1]):\n",
    "                for kdx in range(W_1.shape[0]):\n",
    "                    for ldx in range(W_1.shape[1]):\n",
    "                        path_loss = W_0[idx, jdx].item() + W_1[kdx, ldx].item()\n",
    "                        path_losses.append(path_loss)\n",
    "                        path_indices.append([(idx, jdx), (kdx, ldx)])\n",
    "        path_losses_per_task.append(torch.tensor(path_losses))\n",
    "        path_indices_per_task.append(torch.tensor(path_indices))\n",
    "        \n",
    "        print(torch.tensor(path_losses) == (W_0[ : , None] * W_1.T).flatten())\n",
    "        if task == 0:\n",
    "            return\n",
    "        \n",
    "    return path_losses_per_task, path_indices_per_task, indices_per_task\n",
    "\n",
    "for name, param in models_saved[-1].named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)\n",
    "\n",
    "path_losses_per_task, path_indices_per_task, indices_per_task = get_path_losses_per_task(loss_diffs_per_task, n_tasks)\n",
    "indices_per_task = get_indices_per_task(path_losses_per_task, path_indices_per_task, indices_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def prune_to_obtain_circuit(model, model_indices):\n",
    "    \n",
    "    model_state_dict = model.state_dict()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        indices = model_indices[name]\n",
    "        model_state_dict[name][~indices] = 0.0\n",
    "        \n",
    "    model.load_state_dict(model_state_dict)\n",
    "            \n",
    "    return model\n",
    "\n",
    "models_per_task = []\n",
    "\n",
    "for task in tqdm(range(n_tasks)):\n",
    "    models = []\n",
    "    for model in tqdm(models_saved):\n",
    "        task_model = copy.deepcopy(model)\n",
    "        task_model = prune_to_obtain_circuit(task_model, indices_per_task[task])\n",
    "        models.append(task_model)\n",
    "    models_per_task.append(models)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_curve_for_model(models, steps, n, Ss, n_tasks, test_points, test_batch_sizes, device, dtype):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    losses_subtasks = {}\n",
    "    accuracies_subtasks = {}\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for i in range(n_tasks):\n",
    "        losses_subtasks[str(i)] = list()\n",
    "        accuracies_subtasks[str(i)] = list()\n",
    "    \n",
    "    for model in tqdm(models):\n",
    "        with torch.no_grad():\n",
    "            x_i, y_i = get_batch(n_tasks=n_tasks, n=n, Ss=Ss, codes=list(range(n_tasks)), sizes=test_batch_sizes, device=device, dtype=dtype)\n",
    "            y_i_pred = model(x_i)\n",
    "            labels_i_pred = torch.argmax(y_i_pred, dim=1)\n",
    "            accuracies.append(torch.sum(labels_i_pred == y_i).item() / test_points) \n",
    "            losses.append(loss_fn(y_i_pred, y_i).item())\n",
    "            for i in range(n_tasks):\n",
    "                x_i, y_i = get_batch(n_tasks=n_tasks, n=n, Ss=[Ss[i]], codes=[i], sizes=[test_points_per_task], device=device, dtype=dtype)\n",
    "                y_i_pred = model(x_i)\n",
    "                losses_subtasks[str(i)].append(loss_fn(y_i_pred, y_i).item())\n",
    "                labels_i_pred = torch.argmax(y_i_pred, dim=1)\n",
    "                accuracies_subtasks[str(i)].append(torch.sum(labels_i_pred == y_i).item() / test_points_per_task)\n",
    "    return losses, accuracies, losses_subtasks, accuracies_subtasks\n",
    "\n",
    "losses, accuracies, losses_subtasks, accuracies_subtasks = compute_loss_curve_for_model(models_per_task[0], steps, n, Ss, n_tasks, test_points, test_batch_sizes, device, dtype)\n",
    "\n",
    "plot_losses(losses, 'first_task_with_ablation')\n",
    "plot_accuracies(accuracies, 'first_task_with_ablation')\n",
    "plot_losses(losses_subtasks['0'], 'first_task_with_ablation_subtask_' + str(0))\n",
    "plot_accuracies(accuracies_subtasks['0'], 'first_task_with_ablation_subtask_' + str(0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
